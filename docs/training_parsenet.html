

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Training and using layout detector &mdash; PERO Layout training - documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi-orientation documents" href="multi_orientation.html" />
    <link rel="prev" title="Annotating data for layout detector" href="data_annotation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> PERO Layout training
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data_annotation.html">Annotating data for layout detector</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training and using layout detector</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-training">Model training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-inference">Model inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regression-testing">Regression testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multi_orientation.html">Multi-orientation documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="layout.html">layout package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PERO Layout training</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Training and using layout detector</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/training_parsenet.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="training-and-using-layout-detector">
<h1>Training and using layout detector<a class="headerlink" href="#training-and-using-layout-detector" title="Permalink to this headline">¶</a></h1>
<p>This is a minimal step-by-step guide on training and using the torch implemention of ParseNet layout
detection models, based on <a class="reference external" href="https://arxiv.org/abs/2102.11838">https://arxiv.org/abs/2102.11838</a>. The training pipeline has dependencies on pero-ocr package
so make sure to export it to PYTHONPATH.</p>
<p>A complete example of scripts running augmentation and training on the “universal” dataset as well as
the regression testing pipeline can be found in</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">matylda1</span><span class="o">/</span><span class="n">ikodym</span><span class="o">/</span><span class="n">pero</span><span class="o">/</span><span class="n">ParseNet_torch</span>
</pre></div>
</div>
<div class="section" id="data-preparation">
<h2>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h2>
<p>The training data are pairs of an image file and a corresponding PAGE XML file.
The training script expects the data as a file where each line contains paths to image and page xml and an annotation
mask separated by \t, such as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">image</span><span class="o">.</span><span class="n">jpg</span>  <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">page</span><span class="o">.</span><span class="n">xml</span>   <span class="mi">111</span>
</pre></div>
</div>
<p>The annotation mask marks which annotations are available in the PAGE XML: text height annotations,
text baselines annotations, and text blocks annotations. So a file where only text baselines are available
(for example cBAD samples) would have mask 010.
An example training data file can be found in</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">matylda1</span><span class="o">/</span><span class="n">ikodym</span><span class="o">/</span><span class="n">pero</span><span class="o">/</span><span class="n">ParseNet_torch</span><span class="o">/</span><span class="n">comb</span><span class="o">.</span><span class="n">trn</span>
</pre></div>
</div>
<p>First, the data should be augmented before training using the <a class="reference internal" href="layout.html#module-layout.augment_imgaug" title="layout.augment_imgaug"><code class="xref py py-mod docutils literal notranslate"><span class="pre">layout.augment_imgaug</span></code></a>. It takes the training
file list as an input and generates the desired number of augmented image-pagexml pairs in the target output folder.
It additionally generates a files.lst file which preserves the annotation masks and can then be used direcly for
training. To generate 50 augmented image-layout pairs per sample, run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="s2">&quot;$</span><span class="si">{PERO_PATH}</span><span class="s2">/layout/augment_imgaug.py&quot;</span> <span class="o">--</span><span class="nb">list</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">/</span><span class="nb">list</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span> <span class="o">-</span><span class="n">c</span> <span class="mi">50</span> <span class="o">--</span><span class="n">motion</span><span class="o">-</span><span class="n">blur</span> <span class="kc">False</span> <span class="o">--</span><span class="n">defocus</span><span class="o">-</span><span class="n">blur</span> <span class="kc">False</span>
</pre></div>
</div>
<p>The resulting training file will be located in /path/to/output/files.lst. You can check the augmented images
and tune the augmentation parameters.</p>
</div>
<div class="section" id="model-training">
<h2>Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h2>
<p>Layout detector model definition and training scripts can be found in <a class="reference internal" href="layout.parsenet.html#module-layout.parsenet" title="layout.parsenet"><code class="xref py py-mod docutils literal notranslate"><span class="pre">layout.parsenet</span></code></a>. The training
script handles online data loading and layout maps rendering using <a class="reference internal" href="layout.html#module-layout.dataset" title="layout.dataset"><code class="xref py py-mod docutils literal notranslate"><span class="pre">layout.dataset</span></code></a>. During training,
loss values and several metrics implemented in <a class="reference internal" href="layout.html#module-layout.metrics" title="layout.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">layout.metrics</span></code></a> are saved periodically.
An example of running layout training is</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="s2">&quot;$</span><span class="si">{PERO_PATH}</span><span class="s2">/layout/parsenet/train.py&quot;</span> <span class="o">--</span><span class="n">trn</span><span class="o">-</span><span class="n">data</span> <span class="s2">&quot;/path/to/files.lst&quot;</span> <span class="o">--</span><span class="n">tst</span><span class="o">-</span><span class="n">data</span> <span class="s2">&quot;/path/to/test_files.lst&quot;</span> <span class="o">-</span><span class="n">o</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span> <span class="o">--</span><span class="n">start</span><span class="o">-</span><span class="n">iteration</span> <span class="mi">1</span> <span class="o">--</span><span class="nb">max</span><span class="o">-</span><span class="n">iteration</span> <span class="mi">300000</span> <span class="o">--</span><span class="n">view</span><span class="o">-</span><span class="n">step</span> <span class="mi">5000</span> <span class="o">--</span><span class="n">detection</span><span class="o">-</span><span class="n">net</span> <span class="s1">&#39;detection_UNet_3s_3_32&#39;</span> <span class="o">--</span><span class="n">attention</span><span class="o">-</span><span class="n">net</span> <span class="s1">&#39;attention_UNet_3s_3_16&#39;</span> <span class="o">--</span><span class="n">batch</span><span class="o">-</span><span class="n">size</span> <span class="mi">6</span> <span class="o">--</span><span class="n">patch</span><span class="o">-</span><span class="n">size</span> <span class="mi">256</span> <span class="o">-</span><span class="n">g</span> <span class="mi">0</span>
</pre></div>
</div>
<p>The –view-step argument tells the script how often to save the model checkpoint and logs. During each model saving,
a frozen version is also saved. If training is interrupted or if you want to fine-tune a trained model on different
data, just change the –start-teration argument to the last saved iteration. The –detection-net and –attention-net
arguments should be one of the pre-defined architectures that can be found in <a class="reference internal" href="layout.parsenet.html#module-layout.parsenet.parse_nets" title="layout.parsenet.parse_nets"><code class="xref py py-mod docutils literal notranslate"><span class="pre">layout.parsenet.parse_nets</span></code></a>.
They can also be easily extended to experiment with different model depths.
Afted the training, the final frozen model is in</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">ParseNet</span><span class="o">.</span><span class="n">pt</span>
</pre></div>
</div>
</div>
<div class="section" id="model-inference">
<h2>Model inference<a class="headerlink" href="#model-inference" title="Permalink to this headline">¶</a></h2>
<p>Layout extraction is done using the pero-ocr package. Extracting PAGE XML and rendered layouts from some image files using
a basic configuration file would look like this</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="s2">&quot;$</span><span class="si">{PERO_OCR_PATH}</span><span class="s2">/user_scripts/parse_folder.py&quot;</span> <span class="o">-</span><span class="n">c</span> <span class="s1">&#39;/mnt/matylda1/ikodym/configs/config_torch.ini&#39;</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">image</span><span class="o">-</span><span class="n">path</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">images</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">xml</span><span class="o">-</span><span class="n">path</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">pagexmls</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">render</span><span class="o">-</span><span class="n">path</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">renders</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure you change the path to the layout model in the config file. The rest of the config doesn’t need to be changed.</p>
</div>
</div>
<div class="section" id="regression-testing">
<h2>Regression testing<a class="headerlink" href="#regression-testing" title="Permalink to this headline">¶</a></h2>
<p>It’s not easy to make sure that a new layout model performance doesn’t drop on any type of input image. There is
an evaluation pipeline that compares the resulting layout and OCR output to a reference. Comparing these outputs of
two models can show how they compare to each other even if the reference is not perfect. For an example on how to
run the evaluation pipeline, refer to</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">matylda1</span><span class="o">/</span><span class="n">ikodym</span><span class="o">/</span><span class="n">pero</span><span class="o">/</span><span class="n">ocr_pipeline_eval</span><span class="o">/</span><span class="n">compare_model</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="multi_orientation.html" class="btn btn-neutral float-right" title="Multi-orientation documents" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="data_annotation.html" class="btn btn-neutral float-left" title="Annotating data for layout detector" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Olda Kodym, Michal Hradiš.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>